{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6e13d8-f818-4789-a2bc-ea44dc9a708d",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656c15c-405a-4b82-a47a-483918a00d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from spacy import displacy\n",
    "import uuid\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494c7cb-23d8-4ff0-9da9-088da2ecdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"/lium/home/tprouteau/git/sinr_embeddings/notebooks/afp_5000_firstlines.txt\"\n",
    "OUTPUT_PATH = \"./processed/\"\n",
    "N_JOBS = 20\n",
    "REGISTER = \"web\" # Genre of data (metadata, use whatever you want)\n",
    "LANGUAGE = \"fr\" # Language of the corpus\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\") # Load the model into SpaCy\n",
    "_ = nlp.add_pipe(\"merge_entities\", after=\"ner\") # Merge Named-Entities\n",
    "\n",
    "Path(OUTPUT_PATH).mkdir(exist_ok=True)\n",
    "corpus_output_path = Path(OUTPUT_PATH) / f\"{Path(INPUT_PATH).stem}.vrt\" # Output path to write the corpus file\n",
    "corpus_output_path.touch() # Create the output file\n",
    "with corpus_output_path.open(\"w\") as file:\n",
    "    id_corpus = str(uuid.uuid4()) # Generate a random corpus id\n",
    "    file.write(f'<text id=\"{id_corpus}\" filename=\"{Path(INPUT_PATH).as_posix()}\" register=\"{REGISTER}\" language=\"{LANGUAGE}\">\\n' ) # Write corpus identifier\n",
    "    data_file = Path(INPUT_PATH).open(\"r\")\n",
    "    data = data_file.read().splitlines() # Read INPUT_FILE\n",
    "    print(len(data))\n",
    "    data_file.close()\n",
    "    for doc in tqdm(nlp.pipe(data, n_process=N_JOBS), total=len(data)): \n",
    "        for sent in doc.sents: # Sentence border detection\n",
    "            file.write(\"<s>\\n\") # Write a sentence start\n",
    "            for token in sent: # For each token\n",
    "                if token.ent_type_ == '': # If current token is not a Named-Entity\n",
    "                    ent_type = None\n",
    "                    text = token.text\n",
    "                    lemma = token.lemma_\n",
    "                else:\n",
    "                    ent_type = token.ent_type_\n",
    "                    if ' ' in token.text: # Entities are merged with a space by default\n",
    "                        text = token.text.replace(' ', '_') # We want to merge named entities with a _\n",
    "                        lemma = text\n",
    "                    else:\n",
    "                        text = token.text\n",
    "                        lemma = text\n",
    "                content = \"\\t\".join([text,\n",
    "                                     lemma, \n",
    "                                     token.pos_,  \n",
    "                                     token.ent_iob_, \n",
    "                                     str(ent_type), \n",
    "                                     str(token.is_punct), \n",
    "                                     str(token.is_stop), \n",
    "                                     str(token.is_alpha), \n",
    "                                     str(token.is_digit), \n",
    "                                     str(token.like_num)])\n",
    "                file.write(f'{content}\\n') # Write the token info\n",
    "            file.write(\"</s>\\n\")\n",
    "print(f\"VRT-style file written in {corpus_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c9679-0397-4440-afdf-19b0f90c2728",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b0b9a-20d0-44b0-9f7e-225b6926ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tabulate\n",
    "from collections import Counter\n",
    "def extract_text(text, lemmatize=True, stop_words=False, lower_words=True, number=False, punct=False, exclude_pos=[], en=True, min_freq=10, alpha=True, exclude_en=[], min_length_word=3):\n",
    "    '''corpus_path\n",
    "    Extracts the text from a VRT corpus file.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    corpus_path (str|pathlib.Path): Path to the corpus file.\n",
    "    lemma (bool): Return lemmatized text (default: True).\n",
    "    stop_words (bool): Keep stop-words (default: False).\n",
    "    lower (bool): Put the text in lowercase (default: True).\n",
    "    number (bool): Keep the numbers (default: False).\n",
    "    punct (bool): Keep the punctuation (default: False).\n",
    "    exclude_pos (list): List of part-of speech (from spacy) to exclude) (default: []).\n",
    "    en (bool): Keep named entities (default:True)\n",
    "    min_freq (int): Minimum number of occurrences to keep a token (default: 10).\n",
    "    alpha (bool): Keep alphanumeric characters (default: False).\n",
    "    exclude_en (list): List of named-entities types to exclude (default: []).\n",
    "    \n",
    "    \n",
    "    Return:\n",
    "    text (list(list(str))): A list of sentences containing words\n",
    "    '''\n",
    "    # corpus_file = open_corpus(corpus_path)\n",
    "    # line = corpus.readline().rstrip()\n",
    "    # text = []\n",
    "    out = []\n",
    "    pattern = re.compile(r\"<text[^<>]*\\\"\\>{1}\")\n",
    "    stop_words, number, punct, alpha = str(stop_words), str(number), str(punct), str(alpha)\n",
    "    sentence = []\n",
    "    # x=0\n",
    "    # while line!='' and x<1000:\n",
    "    for line in tqdm(text, total=len(text)):\n",
    "        if line.startswith(\"<s>\"):\n",
    "            sentence = []\n",
    "        elif line.startswith(\"</s>\"):\n",
    "            if len(sentence)>2:\n",
    "                out.append(sentence)\n",
    "        elif len(pattern.findall(line)) > 0:\n",
    "            pass\n",
    "        else:\n",
    "            listline=line.split(\"\\t\")\n",
    "            if len(listline) == 10 :\n",
    "                for i in listline :\n",
    "                    if bool(re.match('^\\t\\t',str(i))) :\n",
    "                        continue\n",
    "                token, lemma, pos, ent_iob, ent_type, is_punct, is_stop, is_alpha, is_digit, like_num = line.split(\"\\t\")\n",
    "                if lemmatize:\n",
    "                    if stop_words==is_stop and is_punct == punct and is_digit == number and like_num == number and not pos in exclude_pos and not ent_type in exclude_en and (alpha == is_alpha or ent_type != \"None\"):\n",
    "                        if exclude_en == True and ent_iob != \"None\":\n",
    "                            pass\n",
    "                        else:\n",
    "                            if lower_words:\n",
    "                                if ent_type != \"None\" and len(lemma)>1:                                \n",
    "                                    sentence.append(token)#sentence.append(lemma.lower())\n",
    "                                    # print(lemma)\n",
    "                                elif len(lemma) > min_length_word:\n",
    "                                    sentence.append(lemma.lower())\n",
    "                            else:\n",
    "                                if ent_type != \"None\":                                \n",
    "                                    sentence.append(token)\n",
    "                                elif len(lemma) > min_length_word:\n",
    "                                    sentence.append(lemma)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    if stop_words==is_stop and is_punct == punct and is_digit == number and alpha == is_alpha and like_num == number and not pos in exclude_pos and not ent_type in exclude_en:\n",
    "                            if exclude_en == True and ent_iob != \"None\":\n",
    "                                pass\n",
    "                            else:\n",
    "                                if lower==True:\n",
    "                                    if ent_type != \"None\" and len(token)>1:                                \n",
    "                                        sentence.append(token) #(token)\n",
    "                                        print(setence)\n",
    "                                    elif len(token) > min_length_word:\n",
    "                                        sentence.append(token.lower())\n",
    "                                    #print(lower(lemma))\n",
    "                                else:\n",
    "                                    if ent_type != \"None\":                                \n",
    "                                        sentence.append(token) #(token)\n",
    "                                    elif len(lemma) > min_length_word:\n",
    "                                        sentence.append(token)\n",
    "            else:\n",
    "                continue\n",
    "    if min_freq > 1:\n",
    "        counts = Counter([word for sent in out for word in sent])\n",
    "        accepted_tokens = {word for word, count in counts.items() if count>=min_freq}\n",
    "        out = [[word for word in sent if word in accepted_tokens] for sent in out]\n",
    "        # line = corpus.readline().rstrip()\n",
    "        # x+=1\n",
    "    return out\n",
    "    \n",
    "def open_corpus(corpus_path):\n",
    "    if isinstance(corpus_path, str):\n",
    "        corpus_file = Path(corpus_path).open(\"r\")\n",
    "    elif isinstance(corpus_path, Path):\n",
    "        corpus_file = corpus_path.open(\"r\")\n",
    "    else:\n",
    "        raise TypeError(f\"Path to corpus : corpus_path must be of type str or pathlib.Path. Provided corpus_path type is {corpus_path.type}\")\n",
    "    return corpus_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631aaf4-b6c5-43a8-8b7a-a6df5e42c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = open_corpus(\"/lium/raid01_c/sguillot/sinr_exps/smalldiach_years_trueprocessed/fatnews.vrt\").read().splitlines() # Load the vrt lines\n",
    "text = extract_text(vrt,lemmatize=True, lower_words=True, min_freq=5) # Extract the text needed\n",
    "_ = [print(sent) for sent in text[:5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinr",
   "language": "python",
   "name": "sinr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
